{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b23f8701-2fe9-4e60-822d-64b4c2014e93",
   "metadata": {},
   "source": [
    "# 3. Training a CLIP-Field\n",
    "\n",
    "In this tutorial, we are going to create a CLIP-Field from our saved data. CLIP-Field is an implicit neural field that maps from 3D XYZ coordinates to higher dimensional representations such as CLIP visual features and Sentence-BERT semantic embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a145e40-59e6-44f9-abcb-f6c20cd921c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import pprint\n",
    "import random\n",
    "from typing import Dict, Union\n",
    "\n",
    "import hydra\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics\n",
    "import tqdm\n",
    "from omegaconf import OmegaConf\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "import wandb\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6b77658-c376-4915-9890-8e6c913228d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "from dataloaders import (\n",
    "    R3DSemanticDataset,\n",
    "    DeticDenseLabelledDataset,\n",
    "    ClassificationExtractor\n",
    ")\n",
    "from misc import ImplicitDataparallel\n",
    "from grid_hash_model import GridCLIPModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0581869-7a7f-4a57-9d8a-187321077cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the constants\n",
    "\n",
    "SAVE_DIRECTORY = \"../clip_implicit_model\"\n",
    "DEVICE = \"cuda\"\n",
    "IMAGE_TO_LABEL_CLIP_LOSS_SCALE = 1.0\n",
    "LABEL_TO_IMAGE_LOSS_SCALE = 1.0\n",
    "EXP_DECAY_COEFF = 0.5\n",
    "SAVE_EVERY = 5\n",
    "METRICS = {\n",
    "    \"accuracy\": torchmetrics.Accuracy,\n",
    "}\n",
    "\n",
    "BATCH_SIZE = 11000\n",
    "NUM_WORKERS = 10\n",
    "\n",
    "CLIP_MODEL_NAME = \"ViT-B/32\"\n",
    "SBERT_MODEL_NAME = \"all-mpnet-base-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d954f230-867e-42e6-814c-0202121afe9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data and create the dataloader created in the previous tutorial notebook\n",
    "\n",
    "#dataset = R3DSemanticDataset(\"../CDSlab.r3d\")\n",
    "#training_data = torch.load(\"../cdslab_labelled_dataset.pt\")\n",
    "training_data = torch.load(\"../detic_labeled_dataset.pt\")\n",
    "#training_data = DeticDenseLabelledDataset(\n",
    "#    dataset, \n",
    "#    use_extra_classes=False, \n",
    "#    exclude_gt_images=False, \n",
    "#    use_lseg=False, \n",
    "#    subsample_prob=0.01, \n",
    "#    visualize_results=True, \n",
    "#    detic_threshold=0.4,\n",
    "#    visualization_path=\"detic_labelled_results\",\n",
    "#)\n",
    "#torch.save(traning_data, \"../sam_labeled_dataset.pt\")\n",
    "max_coords, _ = training_data._label_xyz.max(dim=0)\n",
    "min_coords, _ = training_data._label_xyz.min(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb07bd0-aea0-48d4-b738-505e6de51234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the model\n",
    "\n",
    "label_model = GridCLIPModel(\n",
    "    image_rep_size=training_data[0][\"clip_image_vector\"].shape[-1],\n",
    "    text_rep_size=training_data[0][\"clip_vector\"].shape[-1],\n",
    "    mlp_depth=1,\n",
    "    mlp_width=600,\n",
    "    log2_hashmap_size=20,\n",
    "    num_levels=18,\n",
    "    level_dim=8,\n",
    "    per_level_scale=2,\n",
    "    max_coords=max_coords,\n",
    "    min_coords=min_coords,\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5769f833-7210-457a-8757-1c7e56f2655b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from point2emb import Point2EmbModel\n",
    "label_model = Point2EmbModel(\n",
    "    image_rep_size=training_data[0][\"clip_image_vector\"].shape[-1],\n",
    "    text_rep_size=training_data[0][\"clip_vector\"].shape[-1],\n",
    "    #image_rep_size=512,\n",
    "    #text_rep_size=768,\n",
    "    num_layers = 4, \n",
    "    hidden_dims = 256,\n",
    "    num_pos_embs = 6,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a8685a-c75c-4abd-9603-2871fc948375",
   "metadata": {},
   "source": [
    "## Training and evaulation code\n",
    "\n",
    "Now, we will set up the training and the evaluation code. We will train the model to predict the CLIP/SBert features from the 3D coordinates with a contrastive loss. For evaluation, we will measure the zero-shot label accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f631833-cf0a-4721-a4d6-5ee487935d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def zero_shot_eval(\n",
    "    classifier: ClassificationExtractor, \n",
    "    predicted_label_latents: torch.Tensor, \n",
    "    predicted_image_latents: torch.Tensor, \n",
    "    language_label_index: torch.Tensor, \n",
    "    metric_calculators: Dict[str, Dict[str, torchmetrics.Metric]]\n",
    "):\n",
    "    \"\"\"Evaluate the model on the zero-shot classification task.\"\"\"\n",
    "    class_probs = classifier.calculate_classifications(\n",
    "        model_text_features=predicted_label_latents,\n",
    "        model_image_features=predicted_image_latents,\n",
    "    )\n",
    "    # Now figure out semantic accuracy and loss.\n",
    "    # Semseg mask is necessary for the boundary case where all the points in the batch are \"unlabeled\"\n",
    "    semseg_mask = torch.logical_and(\n",
    "        language_label_index != -1,\n",
    "        language_label_index < classifier.total_label_classes,\n",
    "    ).squeeze(-1)\n",
    "    if not torch.any(semseg_mask):\n",
    "        classification_loss = torch.zeros_like(semseg_mask).mean(dim=-1)\n",
    "    else:\n",
    "        # Figure out the right classes.\n",
    "        masked_class_prob = class_probs[semseg_mask]\n",
    "        masked_labels = language_label_index[semseg_mask].squeeze(-1).long()\n",
    "        classification_loss = F.cross_entropy(\n",
    "            torch.log(masked_class_prob),\n",
    "            masked_labels,\n",
    "        )\n",
    "        if metric_calculators.get(\"semantic\"):\n",
    "            for _, calculators in metric_calculators[\"semantic\"].items():\n",
    "                _ = calculators(masked_class_prob, masked_labels)\n",
    "    return classification_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73012e8b-8a70-4079-bdc3-526f1d5e4b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    clip_train_loader: DataLoader,\n",
    "    labelling_model: Union[GridCLIPModel, ImplicitDataparallel],\n",
    "    optim: torch.optim.Optimizer,\n",
    "    epoch: int,\n",
    "    classifier: ClassificationExtractor,\n",
    "    device: Union[str, torch.device] = DEVICE,\n",
    "    exp_decay_coeff: float = EXP_DECAY_COEFF,\n",
    "    image_to_label_loss_ratio: float = IMAGE_TO_LABEL_CLIP_LOSS_SCALE,\n",
    "    label_to_image_loss_ratio: float = LABEL_TO_IMAGE_LOSS_SCALE,\n",
    "    disable_tqdm: bool = False,\n",
    "    metric_calculators: Dict[str, Dict[str, torchmetrics.Metric]] = {},\n",
    "):\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "    \"\"\"\n",
    "    total_loss = 0\n",
    "    label_loss = 0\n",
    "    image_loss = 0\n",
    "    classification_loss = 0\n",
    "    total_samples = 0\n",
    "    total_classification_loss = 0\n",
    "    labelling_model.train()\n",
    "    total = len(clip_train_loader)\n",
    "    for clip_data_dict in tqdm.tqdm(\n",
    "        clip_train_loader,\n",
    "        total=total,\n",
    "        disable=disable_tqdm,\n",
    "        desc=f\"Training epoch {epoch}\",\n",
    "    ):\n",
    "        xyzs = clip_data_dict[\"xyz\"].to(device)\n",
    "        clip_labels = clip_data_dict[\"clip_vector\"].to(device)\n",
    "        clip_image_labels = clip_data_dict[\"clip_image_vector\"].to(device)\n",
    "        image_weights = torch.exp(-exp_decay_coeff * clip_data_dict[\"distance\"]).to(\n",
    "            device\n",
    "        )\n",
    "        label_weights = clip_data_dict[\"semantic_weight\"].to(device)\n",
    "        image_label_index: torch.Tensor = (\n",
    "            clip_data_dict[\"img_idx\"].to(device).reshape(-1, 1)\n",
    "        )\n",
    "        language_label_index: torch.Tensor = (\n",
    "            clip_data_dict[\"label\"].to(device).reshape(-1, 1)\n",
    "        )\n",
    "\n",
    "        (predicted_label_latents, predicted_image_latents) = labelling_model(xyzs)\n",
    "        # Calculate the loss from the image to label side.\n",
    "        batch_size = len(image_label_index)\n",
    "        image_label_mask: torch.Tensor = (\n",
    "            image_label_index != image_label_index.t()\n",
    "        ).float() + torch.eye(batch_size, device=device)\n",
    "        language_label_mask: torch.Tensor = (\n",
    "            language_label_index != language_label_index.t()\n",
    "        ).float() + torch.eye(batch_size, device=device)\n",
    "\n",
    "        # For logging purposes, keep track of negative samples per point.\n",
    "        image_label_mask.requires_grad = False\n",
    "        language_label_mask.requires_grad = False\n",
    "        contrastive_loss_labels = labelling_model.compute_loss(\n",
    "            predicted_label_latents,\n",
    "            clip_labels,\n",
    "            label_mask=language_label_mask,\n",
    "            weights=label_weights,\n",
    "        )\n",
    "        contrastive_loss_images = labelling_model.compute_loss(\n",
    "            predicted_image_latents,\n",
    "            clip_image_labels,\n",
    "            label_mask=image_label_mask,\n",
    "            weights=image_weights,\n",
    "        )\n",
    "        del (\n",
    "            image_label_mask,\n",
    "            image_label_index,\n",
    "            language_label_mask,\n",
    "        )\n",
    "\n",
    "        # Mostly for evaluation purposes, calculate the classification loss.\n",
    "        classification_loss = zero_shot_eval(\n",
    "            classifier, predicted_label_latents, predicted_image_latents, language_label_index, metric_calculators\n",
    "        )\n",
    "\n",
    "        contrastive_loss = (\n",
    "            image_to_label_loss_ratio * contrastive_loss_images\n",
    "            + label_to_image_loss_ratio * contrastive_loss_labels\n",
    "        )\n",
    "\n",
    "        optim.zero_grad(set_to_none=True)\n",
    "        contrastive_loss.backward()\n",
    "        optim.step()\n",
    "        # Clip the temperature term for stability\n",
    "        labelling_model.temperature.data = torch.clamp(\n",
    "            labelling_model.temperature.data, max=np.log(100.0)\n",
    "        )\n",
    "        label_loss += contrastive_loss_labels.detach().cpu().item()\n",
    "        image_loss += contrastive_loss_images.detach().cpu().item()\n",
    "        total_classification_loss += classification_loss.detach().cpu().item()\n",
    "        total_loss += contrastive_loss.detach().cpu().item()\n",
    "        total_samples += 1\n",
    "\n",
    "    to_log = {\n",
    "        \"train_avg/contrastive_loss_labels\": label_loss / total_samples,\n",
    "        \"train_avg/contrastive_loss_images\": image_loss / total_samples,\n",
    "        \"train_avg/semseg_loss\": total_classification_loss / total_samples,\n",
    "        \"train_avg/loss_sum\": total_loss / total_samples,\n",
    "        \"train_avg/labelling_temp\": torch.exp(labelling_model.temperature.data.detach())\n",
    "        .cpu()\n",
    "        .item(),\n",
    "    }\n",
    "    for metric_dict in metric_calculators.values():\n",
    "        for metric_name, metric in metric_dict.items():\n",
    "            try:\n",
    "                to_log[f\"train_avg/{metric_name}\"] = (\n",
    "                    metric.compute().detach().cpu().item()\n",
    "                )\n",
    "            except RuntimeError as e:\n",
    "                to_log[f\"train_avg/{metric_name}\"] = 0.0\n",
    "            metric.reset()\n",
    "    wandb.log(to_log)\n",
    "    logging.debug(pprint.pformat(to_log, indent=4, width=1))\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f7648e-446b-41bc-bd52-b6bb3b687b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(\n",
    "    labelling_model: Union[ImplicitDataparallel, GridCLIPModel],\n",
    "    optim: torch.optim.Optimizer,\n",
    "    epoch: int,\n",
    "    save_directory: str = SAVE_DIRECTORY,\n",
    "    saving_dataparallel: bool = False,\n",
    "):\n",
    "    if saving_dataparallel:\n",
    "        to_save = labelling_model.module\n",
    "    else:\n",
    "        to_save = labelling_model\n",
    "    state_dict = {\n",
    "        \"model\": to_save.state_dict(),\n",
    "        \"optim\": optim.state_dict(),\n",
    "        \"epoch\": epoch,\n",
    "    }\n",
    "    torch.save(\n",
    "        state_dict,\n",
    "        f\"{save_directory}/implicit_scene_label_model_latest_{epoch}.pt\",\n",
    "    )\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1a2e5c-49dd-4a5a-89a8-3e3a2f90243a",
   "metadata": {},
   "source": [
    "## Set up the auxilary classes\n",
    "\n",
    "Like zero-shot classifier, dataloader, evaluators, optimizer, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7879d6-0845-4166-9cab-23174b772865",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_classifier = ClassificationExtractor(\n",
    "    clip_model_name=CLIP_MODEL_NAME,\n",
    "    sentence_model_name=SBERT_MODEL_NAME,\n",
    "    class_names=training_data._all_classes,\n",
    "    device=DEVICE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04d9da3-ac8a-48e7-aeaa-1b5c4b9ddc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up our metrics on this dataset.\n",
    "train_metric_calculators = {}\n",
    "train_class_count = {\"semantic\": train_classifier.total_label_classes}\n",
    "average_style = [\"micro\", \"macro\", \"weighted\"]\n",
    "for classes, counts in train_class_count.items():\n",
    "    train_metric_calculators[classes] = {}\n",
    "    for metric_name, metric_cls in METRICS.items():\n",
    "        for avg in average_style:\n",
    "            if \"accuracy\" in metric_name:\n",
    "                new_metric = metric_cls(\n",
    "                    num_classes=counts, average=avg, multiclass=True\n",
    "                ).to(DEVICE)\n",
    "                train_metric_calculators[classes][\n",
    "                    f\"{classes}_{metric_name}_{avg}\"\n",
    "                ] = new_metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb15b8e-d279-4bef-9d7f-70f483d8e8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No dataparallel for now\n",
    "batch_multiplier = 1\n",
    "\n",
    "clip_train_loader = DataLoader(\n",
    "    training_data,\n",
    "    batch_size=batch_multiplier * BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")\n",
    "logging.debug(f\"Total train dataset sizes: {len(training_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6998a969-23ce-4c1c-9e1e-f50cadfa9cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up optimizer\n",
    "\n",
    "optim = torch.optim.Adam(\n",
    "    label_model.parameters(),\n",
    "    lr=1e-4,\n",
    "    betas=(0.9, 0.999),\n",
    "    weight_decay=0.003,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa708739-7aa1-4056-884c-b8d7ddaa3ca2",
   "metadata": {},
   "source": [
    "## Model training\n",
    "\n",
    "Now we run our training loop and save the model occassionally. We ran this for 5 epochs just to validate everything is working properly, but to train a full model you should train it for longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2235b9cc-74c0-46c9-b399-438854fbbf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.init(\n",
    "    project=\"clip_field\"\n",
    ")\n",
    "# Set the extra parameters.\n",
    "wandb.config.web_labelled_points = len(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419d9a54-f872-4a28-a72f-3ef4f17b8814",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_model = label_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a19e9d-f16a-4443-a3aa-5aec944fa7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\" # Just to reduce excessive logging from sbert\n",
    "\n",
    "epoch = 0\n",
    "NUM_EPOCHS = 100\n",
    "SAVE_EVERY = 5\n",
    "\n",
    "while epoch <= NUM_EPOCHS:\n",
    "    train(\n",
    "        clip_train_loader,\n",
    "        label_model,\n",
    "        optim,\n",
    "        epoch,\n",
    "        train_classifier,\n",
    "        metric_calculators=train_metric_calculators,\n",
    "        #label_to_image_loss_ratio = 0.0\n",
    "    )\n",
    "    epoch += 1\n",
    "    if epoch % SAVE_EVERY == 0:\n",
    "        save(label_model, optim, epoch, save_directory = '../detic_label_regularized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4938a568-b0bb-4043-bcf0-3be3804ad0b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
